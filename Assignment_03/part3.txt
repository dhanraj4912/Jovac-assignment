1. What are the assumptions of linear regression?
Linear regression assumes there’s a straight-line relationship between the input and output variables. It also expects the residuals (errors) to have constant variance (homoscedasticity), to be independent, and to follow a normal distribution. Plus, the predictors shouldn’t be highly correlated with each other (no multicollinearity).

2. When should you use logistic regression instead of linear regression?
You should use logistic regression when your target variable is categorical, especially for binary outcomes like yes/no or 0/1. Unlike linear regression, it’s designed to predict probabilities and classify them into categories using a sigmoid function.

3. What is the interpretation of coefficients in logistic regression?
Each coefficient shows how a one-unit increase in a predictor changes the log-odds of the outcome, keeping other variables constant. If you exponentiate the coefficient, you get the odds ratio, which is often easier to interpret.

4. What is the difference between sigmoid and softmax functions?
The sigmoid function squeezes any number into a range between 0 and 1, making it perfect for binary classification. Softmax, on the other hand, is for multiclass problems—it turns a list of numbers into probabilities that add up to 1 across all classes.

5. Why is R-squared not suitable for evaluating logistic regression models?
R-squared works for linear models with continuous targets, but logistic regression predicts probabilities instead. Metrics like accuracy, precision, recall, F1-score, AUC, or log-loss are better suited for evaluating its performance.

